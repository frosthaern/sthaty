# sthaty

## TODO

- load the dataset
- convert the dataset to list of strings
- tokenize, vectorize and extract attention heads for all of them
- now i just want to figure out what index are the special tokens are in so that i can slice that part
- i am returning a dict of tensor just have those things also in the dict ( the indexes )
- anyways i have to move it forward till the attention extraction phase


- and you have to apply jensen_coeff for all of them and store it in a file and have the filename
- then you have to use the jensen heads and cluster, you can use something that uses cosine similarity
- then you have to calculate cluster head and calculate score for each cluster and heatmap for each cluster
- and visualize the activation heads for each cluster
